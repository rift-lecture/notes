\chapter{Automatic Memory Management}

Programs need physical memory to store their data. Between the idealized runtime representation of data in a high-level language and the actual physical, localized storage we find several layers of indirection. For the purpose of this chapter we will take the memory management provided by the Operating System as a given and build on top of it to provide memory to \rift programs.

\section{Problem Statement}

Considering the simple C++ program given in \autoref{lst:mem-cpp} we can observe various strategies on how to store program state.

\begin{lstlisting}[language=cpp, caption={Memory in C++}, label={lst:mem-cpp}]
void aFunction() {
  int i = 0;
  Vector b(1, 2, 3);
  Vector * c = new Vector(1, 2, 3);
  globalVector = c;
}

void main() {
  aFunction();
  delete globalVector;
}
\end{lstlisting}

The first allocation on line 2 is trivial to translate, since it conveniently maps without further complications to a machine register. The Vector $b$ created on line 3 is a bit more complicated. C++ semantics dictate that the vector is not to be accessed after the closing bracket on line 6, \ie we say it goes out of scope. A natural choice for a compiler would be to store it inside the current activation frame of the runtime stack. Finally $c$ created on line 4 is allocated using the $new$ operator. Hereby we request the compiler to allocate it in a more permanent fashion, so we can reuse it outside the current function scope.

\begin{figure}[h]
\centering
\includegraphics[width=0.2\textwidth]{runtime_stack}
\caption{Runtime stack when aFunction from \autoref{lst:mem-cpp} is activated.}
\label{fig:runtime-stack}
\end{figure}

A possible choice for an activation frame of aFunction from \autoref{lst:mem-cpp} is given in \autoref{fig:runtime-stack}. Assuming the variable $i$ had to be stored permanently at some point we can see the value of i and the contents of the vector $b$ stored inside the current frame. Note that to release all the memory upon returning from the function it suffices to adjust the stack pointer (SP). This allows us to reuse the memory without further overhead. Finally the variable $c$ is not stored on the stack, rather we only keep a pointer (denoted $*c$) and store the actual value outside the stack. We call this \emph{heap allocation}. Storage allocated on the heap is not reclaimed by returning from the function. This allows us to use the value in a higher up, \ie earlier frame; or as we can see in \autoref{lst:mem-cpp} on line 5 to assign it to a global value. In C++ heap objects are not automatically reclaimed and the programmer is responsible to manually delete the object when its no longer in use.

It might seem that we could write most programs to just use heap allocated memory and thereby avoid the cognitive overhead of requesting and releasing memory. In practice most programming languages would want to provide heap storage to the user. In particular stack allocated memory is

\begin{description}
  \item[limited] The runtime stack needs to be a linear address space which is more expensive to provide than a segmented address spaces.
  \item[local] We might be allowed to pass references to values inside our own call frame to later frames, but it will never be possible to return a reference. Thus all return values need to passed by value, \ie copied down to the previous function which is expensive.
\end{description}

\subsection{Allocation Sites in \rift Code}

If we consider \rift code the situation is a bit different. Lets consider the code in \autoref{lst:mem-rift} and the corresponding memory which is allocated as shown in \autoref{fig:rift_stack}.

\begin{lstlisting}[language=rift, caption={Some values allocated in \rift.}, label={lst:mem-rift}]
function(a) {
  b <- 1
  b <- a + b
}
\end{lstlisting}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{rift_stack}
\caption{Memory layout of a \rift function given in \autoref{lst:mem-rift}.}
\label{fig:rift_stack}
\end{figure}

Unlike in the C++ case all \rift values are allocated on the stack. This releases the programmer from the cognitive burden to figure out where to put its values. In the spirit of a dynamic language he can pass any value to any function and of course also return it. The runtime stack is only used to keep track of environments, which are the runtime representation of \rift stack frames.

Of course this creates a problem. As demonstration we run a very simple \rift program given in \autoref{lst:rift-mem-usg}. 

\begin{lstlisting}[language=rift, caption={loop.ri}, label={lst:rift-mem-usg}]
a <- 100000
b <- 1
while(a > 0) {
  b <- b + a
  a <- a - 1
}
b
\end{lstlisting}

We can record the memory usage of this program using the \emph{massif} tool of \emph{valgrind} as shown in \autoref{lst:massif}.

\begin{lstlisting}[language=bash, label={lst:massif}]
valgrind --tool=massif ./rift loop.ri
\end{lstlisting}

\emph{Massif} writes a log file of all allocations. We visualize it with the massif-visualizer tool. As we can see in \autoref{fig:massif-blowup} the heap size grows unbounded even though a constant amount of memory would be enough to hold all state of the code in \autoref{lst:rift-mem-usg}. It becomes very obvious that all our runtime functions, \eg $doubleGt$ or $genericAdd$ leak all memory they allocate.

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{massif-blowup}
\caption{Memory consumption of a \rift program given in \autoref{lst:rift-mem-usg} without any memory management enabled.}
\label{fig:massif-blowup}
\end{figure}

The natural question arises of how to deal with the problem. We could be tempted to introduce the delete operation into the \rift language. This would probably be quite hard to achieve: Many \rift statements implicitly allocate as a side effect and we never really get hold of the allocated memory. Consider \eg the $if$ statement, which will evaluate the boolean value of the condition. \Rift semantics require this comparison to allocate a numerical result vector. It would be quite unpleasant to have to clean up all those implicitly created vectors explicitly. Thus we will evaluate our options in the following chapters.

\section{Strategies}

In our language we want to be able to run programs for a long time without unnecessarily running out of memory. But on the other hand we do not want the programmer to be bothered with keeping track of allocations.

Therefore we will design the system in a way which causes all unused memory to be reclaimed automatically without user intervention. There are quite a few options available which we will discuss.

\subsection{Type System Based}

We could try to determine the lifetime of all objects in our system at compile time. There are some recent advances in this direction. E.g. the \emph{rust} language uses a notion of ownership. Every object has to be owned by another one, and it will be automatically removed when its owner gets removed. \emph{Rust} features a type system employing ownership types to statically enforce correct behavior. Whenever you want to pass an object around you have to explicitly transfer its ownership.

While this is certainly a nice approach, in the spirit of our dynamic, low cognitive overhead, easy on the programmer language we do however not want to bother the programmer with figuring out the ownership relations between his objects. We must therefore look at ways to clean up the memory at runtime.

\subsection{Reference Counting}

A simple technique employed by many language implementations is called \emph{reference counting}. The core idea is to count the number of references existing in the system for every object. When we assign the same value to a second variable the counter increases. When a reference goes out of scope it decreases. When the counter goes to zero the object is no longer used and can be removed.

While simple this idea has several drawbacks. First of all it introduces a fairly high runtime overhead to the program. Updating as well as looking up the counters is expensive. Second the technique is not able to free objects if they have cyclic references. Consider a group of several dead objects -- completely unreachable from the current program state -- but \eg placed in a circular buffer. Since they all point to the next object the circle closes and each of them has a reference count of one and will therefore not be freed.

\subsection{Tracing Garbage Collector}

Instead of keeping track of all the dependencies all the time the idea of \emph{tracing garbage collection} -- sometimes referred to as just \emph{garbge collection} (GC) -- is to only interrupt when running out of memory. At the core we have the following (conservative) assumption:

\emph{Everything which is reachable is still used}.

\begin{description}
  \item[Why is is conservative?] Sometimes programs -- either because of the scoping rules of the language or because of programming errors -- keep references to objects which they don't use anymore.
  \item[Is the assumption sound?] Depending on the language. Some languages feature raw pointers and a program might read/write to computed memory locations. E.g. in a C++ program it is possible to reconstruct a pointer from an arithmetic operation like: $(int*)(i+20)$. In \rift however the assumption is sound since all references are managed by the VM.
\end{description}

On a side note it might be interesting to ask what kind of runtime overhead GC introduces. It is almost impossible to come up with a simple measurement since it heavily depends on the workload, on the language, on the implementation of the language and so on. In practice an overhead between a few percents and up to 20 percent are observed, even in well tuned implementations.

Before we look at some algorithms to perform a GC let us define the notion of reachability in a more precise way.

\subsection{Reachability}

Let $b$ be called \emph{reachable in one step from} $a$, written as $a \rightarrow^{1} b$, \emph{iff} $a$ denotes an object containing a reference to the object $b$. We call $b$ \emph{reachable from} $a$, written as $a \rightarrow^{*} b$, \emph{iff} $\exists c_1 ... c_n : a \rightarrow^{1} c_1 \rightarrow^{1} ... \rightarrow^{1} c_n \rightarrow^{1} b$, \ie we define reachability as the transitive closure of having a reference to an object. Finally we call an object $b$ reachable, written as $\rightarrow^{*} b$ \emph{iff} $R \rightarrow^{*} b$ where $R$ denotes the \emph{root set} of live objects.

In a typical environment the \emph{root set} contains \eg global variables, symbols, events and prominently \emph{all the pointers on the runtime stack}. The stack potentially contains temporary values which do not have an assigned name, are not stored in any object yet, but will be used by later instructions. We are therefore often required to assume every pointer on the runtime stack to be part of the root set.

\subsection{Timing}

In the context of the GC we call the running program the mutator, since it is mutating the object graph outside the GC control. We define a \emph{stop the world GC} to be a one which completely halts all mutator threads, then performs the GC and finally resumes the mutator. As we see in \autoref{fig:gc_timing} another option is to run certain parts of GC concurrent to the program. In this case we need to ensure that the collector does not interfere with the mutator and that the mutator does not invalidate the work already done by the collector in an unsafe way.

\begin{figure}[h]
\centering
\includegraphics[width=0.6\textwidth]{gc_timing}
\caption{In its simplest form the program execution (gray) is completely stopped while the GC (black) is running. More advanced would be to let certain parts of the GC run in parallel to the normal program.}
\label{fig:gc_timing}
\end{figure}

\subsection{Strategies}

Finally we are ready to look at two representative GC algorithms. Our main focus will be on the classical \emph{mark and sweep} algorithm. We will also briefly look at \emph{mark and copy}.

\subsubsection{Mark and Sweep}

A mark and sweep GC has two phases. First it marks every reachable object on the heap. Secondly it sweeps the whole heap and reclaims all non-marked memory. The marking phase is recursively visiting every object in the heap. The GC needs to have precise knowledge of the object layout. Starting from the root set it will recursively visit all reachable references. Marking can be implemented in several ways. In the simplest form every object has some common metadata in the beginning, called an object header, where at least one bit is reserved to store the marking information. The sweeping phase will scan through all reserved memory pages and record all continuous blocks of free space by \eg storing them in a freelist. 

\subsubsection{Mark and Copy}

A mark and copy GC will move live objects to achieve a more compact heap layout and provide faster allocation. In its simplest form a so called semispace collector will divide the heap into two halves. Allocation happens exclusively in one of the two halves. When it is full the GC will first perform a normal marking phase, like in the mark and sweep scenario described before. Subsequently all live objects will be copied to the second half. We call the old half the fromspace and the other half the tospace. The copying will result in all live objects to be compactly aligned at the beginning of tospace. After copying the role of to- and fromspace swap. New objects are allocated in the (continuous) free part of the former tospace.

Copying garbage collectors move around objects on the heap. They are therefore required to fix all references pointing to the moved objects, thus need even more precise information on all pointer locations in the system. Often the cost of copying can be amortized by faster allocation, reduced fragmentation and better locality of the heap.

\subsection{Stack Scanning}

As discussed before, many temporary pointers exist solely on the runtime stack. Therefore a GC must be able to scan the runtime stack and discover all the pointers. There are generally two strategies:

\begin{description}
  \item[Precise Stackmaps] For every callsite the compiler produces a precise inventory of all pointer locations. Such a data structure is called stackmap. When unrolling the runtime stack frame by frame the GC queries the stackmap for the current program counter value to know which slots in the activation frame contain a valid object pointer.
  \item[Conservative Marking] An alternative solution without requiring compiler support is to treat every value on the stack as a potential pointer using a conservative marking scheme. The disadvantages of this approach are that we might consider random values to be valid pointers. Also for every pointer candidate we have to be able to answer if it conceivably points to a valid object or not, before we can mark it. On the other hand we do not place any restrictions on the stack layout and need no additional support from the compiler.  
\end{description}

\section{\Rift implementation}

Our \rift implementations features a naive mark and sweep GC using conservative stack scanning. Although LLVM would provide infrastructure to generate precise stack maps we refrained from using it. First of all it would be too complex to fit into the constraints of the lecture. Second the GC support in LLVM has yet to mature to be reasonably usable. Therefore \rift features a conservative stack scan.

In gc.h we find the high level marking algorithm implemented in \autoref{lst:gc1}. The actual marking depends of course on the type of object. As we can see in \autoref{lst:gc2} of the RVal types only the function contains further references. E.g. the double vector only contains data values. In the case of the environment we have to follow all the stored bindings.

\begin{lstlisting}[language=cpp, caption={Generic marking}, label={lst:gc1}]
// Generic mark algorithm is to check if an object was marked before. Iff
// not we will mark it first (to deal with cycles) and then delegate to the
// user defined markImpl<T> for the specific type to scan its child nodes.
template <typename T>
void mark(T * obj, NodeIndex & i) {
    auto & a = arena<T>();
    if (!a.isMarked(i)) {
        a.setMark(i);
        markImpl<T>(obj);
    }
}
\end{lstlisting}

\begin{lstlisting}[language=cpp, caption={Marking for all object types.}, label={lst:gc2}]
template<>
void GarbageCollector::markImpl<RVal>(RVal * value) {
    if (value->type == RVal::Type::Function) {
        mark(value->f->env);
    }   
    // Leaf node, nothing to do
};

template<>
void GarbageCollector::markImpl<Environment>(Environment * env) {
    for (int i = 0; i < env->size; ++i) {
        mark(env->bindings[i].value);
    }   
    if (env->parent)
        mark(env->parent);
}
\end{lstlisting}

The conservative stack scan is shown in \autoref{lst:gc3}. We use a bit of inline assembly on line 4 to make sure all register content id spilled to the runtime stack where our stack scan can pick it up. We see on line 27 that we call $markMaybe$ which is a special mark function which first validates if the pointer is valid.

\begin{lstlisting}[language=cpp, caption={Conservative stack scan.}, label={lst:gc3}]
void scanStack() {
    // Clobber all registers:
    // -> forces all variables currently hold in registers to be spilled
    //    to the stack where our stackScan can find them.
    __asm__ __volatile__ ( "nop" : :
            : "%rax", "%rbx", "%rcx", "%rdx", "%rsi", "%rdi",
            "%r8", "%r9", "%r10", "%r11", "%r12",
            "%r13", "%r14", "%r15");
    _scanStack();
}

// Performance Hack: we assume the first page of memory not to contain any
// heap pointers. Maybe there is also a reasonable upper limit?
bool maybePointer(void* p) {
    return p > (void*)4096;
}

const void * BOTTOM_OF_STACK;

// The stack scan traverses the memory of the C stack and looks at every
// possible stack slot. If we find a valid heap pointer we mark the
// object as well as all objects reachable through it as live.
void __attribute__ ((noinline)) _scanStack() {
    void ** p = (void**)__builtin_frame_address(0);
    while (p < BOTTOM_OF_STACK) {
        if (maybePointer(*p))
            markMaybe(*p);
        p++;
    }
}
\end{lstlisting}

To make sure we can identify valid pointers all objects are allocated in bulk pages (see \autoref{lst:gc5}). The pages are indexed in a so called Arena, which we can query to find out if a pointer is valid. As shown in \autoref{lst:gc4} the Arena and the pages together have enough information about all objects in the system, that we can answer whether a random integer number corresponds to a valid pointer.

\begin{lstlisting}[language=cpp, caption={Finding out if a random value is a valid pointer.}, label={lst:gc4}]
NodeIndex Arena::findNode(void* ptr) {
    uintptr_t addr = reinterpret_cast<uintptr_t>(ptr);
    
    // Simple performance hack, which allows us to quickly discard
    // implausible pointers without iterating the page list.
    if (addr < minAddr || addr > maxAddr)
        return NodeIndex::nodeNotFound();

    for (index_t i = 0; i < page.size(); i++) {
        if (page[i]->isValidObj(ptr)) {
            return NodeIndex(i, page[i]->getIndex(ptr));
        }
    }

    return NodeIndex::nodeNotFound();
}
\end{lstlisting}

\begin{lstlisting}[language=cpp, caption={Overriding the new oparator.}, label={lst:gc5}]
template <typename OBJECT>
struct HeapObject {
    // Overriding new makes sure memory is allocated through the GC
    static void* operator new(size_t sz) {
        return gc::GarbageCollector::alloc<OBJECT>();
    }

    void operator delete(void*) = delete;
};

template <typename T>
void* doAlloc() {
    auto res = arena<T>().alloc();
    if (!res) {
        doGc();
        res = arena<T>().alloc();
    }
    return res;
}
\end{lstlisting}

After adding the GC to the system we revisit the heap usage of our loop.ri program from \autoref{lst:rift-mem-usg} and as we can see in \autoref{fig:massif-gc} the memory consumption stays linear, as it would be expected from this artifact.

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{massif-gc}
\caption{Memory consumption of a \rift program given in \autoref{lst:rift-mem-usg} with GC enabled.}
\label{fig:massif-gc}
\end{figure}

